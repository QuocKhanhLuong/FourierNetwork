{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4d6004",
   "metadata": {},
   "source": [
    "# ðŸ§  EGM-Net: Energy-Gated Gabor Mamba Network\n",
    "\n",
    "**Medical Image Segmentation with Implicit Neural Representations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b879d899",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6517affa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install -q torch torchvision numpy matplotlib tqdm gdown nibabel scikit-image monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed28aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ–¥ï¸ Device: cuda\n",
      "   GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os, glob, json\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"ðŸ–¥ï¸ Device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed39886",
   "metadata": {},
   "source": [
    "---\n",
    "## 2ï¸âƒ£ Model Architecture\n",
    "\n",
    "All model code is defined inline below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3dda03",
   "metadata": {},
   "source": [
    "### 2.1 Mamba Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e5665c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 64, 64, 64])\n",
      "Output shape: torch.Size([2, 64, 64, 64])\n",
      "Module parameters: 31648\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "class DepthwiseSeparableConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2, groups=in_channels, bias=False\n",
    "        )\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=True)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class DirectionalScanner(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, scan_dim: int = 64):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.scan_dim = scan_dim\n",
    "        \n",
    "        # Learnable projection to scan_dim for each direction\n",
    "        self.proj_in = nn.Linear(channels, scan_dim)\n",
    "        \n",
    "        # GRU cell for sequential state processing (simulates SSM)\n",
    "        self.gru_cell = nn.GRUCell(scan_dim, scan_dim)\n",
    "        \n",
    "        # Project back to original channels\n",
    "        self.proj_out = nn.Linear(scan_dim, channels)\n",
    "        \n",
    "    def _scan_direction(self, x: torch.Tensor, direction: str) -> torch.Tensor:\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Prepare sequence based on direction\n",
    "        if direction == \"right\":\n",
    "            # Scan left-to-right: (B, H*W, C) after reshape\n",
    "            x = x.permute(0, 2, 3, 1).reshape(B * H, W, C)  # (B*H, W, C)\n",
    "        elif direction == \"down\":\n",
    "            # Scan top-to-bottom\n",
    "            x = x.permute(0, 3, 2, 1).reshape(B * W, H, C)  # (B*W, H, C)\n",
    "        elif direction == \"left\":\n",
    "            # Scan right-to-left (reverse)\n",
    "            x = x.permute(0, 2, 3, 1).flip(1).reshape(B * H, W, C)  # (B*H, W, C)\n",
    "        elif direction == \"up\":\n",
    "            # Scan bottom-to-top (reverse)\n",
    "            x = x.permute(0, 3, 2, 1).flip(1).reshape(B * W, H, C)  # (B*W, H, C)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown direction: {direction}\")\n",
    "        \n",
    "        # Project to scan dimension\n",
    "        x = self.proj_in(x)  # (*, W/H, scan_dim)\n",
    "        \n",
    "        # Apply GRU cell sequentially (simulates SSM forward pass)\n",
    "        outputs = []\n",
    "        h = torch.zeros(x.shape[0], self.scan_dim, device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        for t in range(x.shape[1]):\n",
    "            h = self.gru_cell(x[:, t], h)  # GRU step\n",
    "            outputs.append(h)\n",
    "        \n",
    "        x = torch.stack(outputs, dim=1)  # (*, W/H, scan_dim)\n",
    "        \n",
    "        # Project back to original channels\n",
    "        x = self.proj_out(x)  # (*, W/H, C)\n",
    "        \n",
    "        # Reshape back to (B, C, H, W)\n",
    "        if direction == \"right\":\n",
    "            x = x.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        elif direction == \"down\":\n",
    "            x = x.reshape(B, W, H, C).permute(0, 3, 2, 1)\n",
    "        elif direction == \"left\":\n",
    "            x = x.reshape(B, H, W, C).permute(0, 3, 1, 2).flip(-1)\n",
    "        elif direction == \"up\":\n",
    "            x = x.reshape(B, W, H, C).permute(0, 3, 2, 1).flip(-2)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Scan in all 4 directions\n",
    "        scan_right = self._scan_direction(x, \"right\")\n",
    "        scan_down = self._scan_direction(x, \"down\")\n",
    "        scan_left = self._scan_direction(x, \"left\")\n",
    "        scan_up = self._scan_direction(x, \"up\")\n",
    "        \n",
    "        # Aggregate by averaging\n",
    "        output = (scan_right + scan_down + scan_left + scan_up) / 4.0\n",
    "        \n",
    "        return output\n",
    "\n",
    "class VSSBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, hidden_dim: Optional[int] = None, \n",
    "                 scan_dim: int = 64, expansion_ratio: float = 2.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        hidden_dim = hidden_dim or int(channels * expansion_ratio)\n",
    "        \n",
    "        # Preprocessing: expand channels\n",
    "        self.norm1 = nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6)\n",
    "        self.conv_expand = nn.Conv2d(channels, hidden_dim, kernel_size=1, bias=True)\n",
    "        \n",
    "        # Directional scanning\n",
    "        self.scanner = DirectionalScanner(hidden_dim, scan_dim=scan_dim)\n",
    "        \n",
    "        # Postprocessing: contract channels back\n",
    "        self.norm2 = nn.GroupNorm(num_groups=32, num_channels=hidden_dim, eps=1e-6)\n",
    "        self.conv_contract = nn.Conv2d(hidden_dim, channels, kernel_size=1, bias=True)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        residual = x\n",
    "        \n",
    "        # Preprocessing\n",
    "        x = self.norm1(x)\n",
    "        x = self.conv_expand(x)\n",
    "        x = F.gelu(x)\n",
    "        \n",
    "        # Directional scanning (core SSM-like operation)\n",
    "        x = self.scanner(x)\n",
    "        \n",
    "        # Postprocessing\n",
    "        x = self.norm2(x)\n",
    "        x = self.conv_contract(x)\n",
    "        \n",
    "        # Residual connection\n",
    "        output = x + residual\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MambaBlockStack(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, depth: int = 2, **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VSSBlock(channels, **kwargs) for _ in range(depth)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test VSSBlock\n",
    "    batch_size, channels, height, width = 2, 64, 64, 64\n",
    "    x = torch.randn(batch_size, channels, height, width)\n",
    "    \n",
    "    vss_block = VSSBlock(channels, scan_dim=32)\n",
    "    output = vss_block(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Module parameters: {sum(p.numel() for p in vss_block.parameters())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0488aab1",
   "metadata": {},
   "source": [
    "### 2.2 Spectral Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1c8fd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 64, 64, 64])\n",
      "Output shape: torch.Size([2, 64, 64, 64])\n",
      "Module parameters: 270336\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "class SpectralGating(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, height: int, width: int, \n",
    "                 threshold: float = 0.1, complex_init: str = \"kaiming\"):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        # Create learnable complex weights for frequency domain\n",
    "        # Shape: (channels, height, width//2 + 1) for rfft2\n",
    "        self.register_buffer(\n",
    "            \"freq_shape\",\n",
    "            torch.tensor([channels, height, width // 2 + 1], dtype=torch.long)\n",
    "        )\n",
    "        \n",
    "        # Real and Imaginary parts of complex weights\n",
    "        self.weight_real = nn.Parameter(\n",
    "            torch.zeros(channels, height, width // 2 + 1)\n",
    "        )\n",
    "        self.weight_imag = nn.Parameter(\n",
    "            torch.zeros(channels, height, width // 2 + 1)\n",
    "        )\n",
    "        \n",
    "        self._init_weights(complex_init)\n",
    "        \n",
    "    def _init_weights(self, strategy: str = \"kaiming\"):\n",
    "        \n",
    "        if strategy == \"identity\":\n",
    "            # Initialize close to identity (magnitude ~1, phase ~0)\n",
    "            nn.init.ones_(self.weight_real)\n",
    "            nn.init.zeros_(self.weight_imag)\n",
    "        elif strategy == \"kaiming\":\n",
    "            # Kaiming initialization adapted for complex numbers\n",
    "            fan_in = self.height * (self.width // 2 + 1)\n",
    "            std = (2.0 / fan_in) ** 0.5\n",
    "            nn.init.normal_(self.weight_real, 0, std)\n",
    "            nn.init.normal_(self.weight_imag, 0, std)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown init strategy: {strategy}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Apply FFT to convert to frequency domain\n",
    "        # rfft2 returns complex tensor\n",
    "        x_freq = torch.fft.rfft2(x, dim=(-2, -1), norm=\"ortho\")\n",
    "        \n",
    "        # Create complex weight matrix: weight_real + 1j * weight_imag\n",
    "        # Reshape to (1, C, H, W//2+1) for broadcasting\n",
    "        complex_weight = (\n",
    "            self.weight_real.unsqueeze(0) + \n",
    "            1j * self.weight_imag.unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        # Apply channel-wise multiplication in frequency domain\n",
    "        # Shape: (B, C, H, W//2+1) * (1, C, H, W//2+1) -> (B, C, H, W//2+1)\n",
    "        x_filtered = x_freq * complex_weight\n",
    "        \n",
    "        # Optional: Hard thresholding to remove low-amplitude noise\n",
    "        if self.threshold > 0:\n",
    "            magnitude = torch.abs(x_filtered)\n",
    "            mask = magnitude > self.threshold\n",
    "            x_filtered = x_filtered * mask.float()\n",
    "        \n",
    "        # Apply inverse FFT to return to spatial domain\n",
    "        output = torch.fft.irfft2(x_filtered, s=(H, W), dim=(-2, -1), norm=\"ortho\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "class FrequencyLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, weight: float = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Apply FFT\n",
    "        pred_freq = torch.fft.rfft2(pred, dim=(-2, -1), norm=\"ortho\")\n",
    "        target_freq = torch.fft.rfft2(target, dim=(-2, -1), norm=\"ortho\")\n",
    "        \n",
    "        # Compute L2 distance in frequency domain\n",
    "        # Using both magnitude and phase information\n",
    "        loss_real = F.mse_loss(pred_freq.real, target_freq.real)\n",
    "        loss_imag = F.mse_loss(pred_freq.imag, target_freq.imag)\n",
    "        \n",
    "        return loss_real + loss_imag\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test SpectralGating\n",
    "    batch_size, channels, height, width = 2, 64, 64, 64\n",
    "    x = torch.randn(batch_size, channels, height, width)\n",
    "    \n",
    "    spec_gate = SpectralGating(channels, height, width)\n",
    "    output = spec_gate(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Module parameters: {sum(p.numel() for p in spec_gate.parameters())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d867eeb",
   "metadata": {},
   "source": [
    "### 2.3 Monogenic Signal Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b7daa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Monogenic Signal Processing...\n",
      "Input shape: torch.Size([1, 1, 128, 128])\n",
      "Energy map shape: torch.Size([1, 1, 128, 128])\n",
      "Energy range: [0.000, 1.000]\n",
      "Monogenic components: ['amplitude', 'phase', 'orientation', 'riesz_x', 'riesz_y']\n",
      "Boundary map shape: torch.Size([1, 1, 128, 128])\n",
      "Boundary range: [0.000, 1.000]\n",
      "\n",
      "âœ“ All tests passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class RieszTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, epsilon: float = 1e-8):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Create frequency grid\n",
    "        freq_y = torch.fft.fftfreq(H, device=x.device, dtype=x.dtype)\n",
    "        freq_x = torch.fft.fftfreq(W, device=x.device, dtype=x.dtype)\n",
    "        freq_y, freq_x = torch.meshgrid(freq_y, freq_x, indexing='ij')\n",
    "        \n",
    "        # Compute radial frequency (avoid division by zero)\n",
    "        radius = torch.sqrt(freq_x**2 + freq_y**2 + self.epsilon)\n",
    "        \n",
    "        # Riesz kernels in frequency domain\n",
    "        # H1 = -j * u / |w|, H2 = -j * v / |w|\n",
    "        kernel_x = freq_x / radius\n",
    "        kernel_y = freq_y / radius\n",
    "        \n",
    "        # Set DC component to zero\n",
    "        kernel_x[0, 0] = 0\n",
    "        kernel_y[0, 0] = 0\n",
    "        \n",
    "        # Apply FFT to input\n",
    "        x_fft = torch.fft.fft2(x)\n",
    "        \n",
    "        # Apply Riesz kernels (multiplication by -j in frequency = Hilbert-like)\n",
    "        # -j * X = real(X) * (-j) + imag(X) * (-j) * j = imag(X) - j*real(X)\n",
    "        riesz_x_fft = -1j * x_fft * kernel_x.unsqueeze(0).unsqueeze(0)\n",
    "        riesz_y_fft = -1j * x_fft * kernel_y.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Inverse FFT\n",
    "        riesz_x = torch.fft.ifft2(riesz_x_fft).real\n",
    "        riesz_y = torch.fft.ifft2(riesz_y_fft).real\n",
    "        \n",
    "        return riesz_x, riesz_y\n",
    "\n",
    "class LogGaborFilter(nn.Module):\n",
    "\n",
    "    def __init__(self, num_scales: int = 4, num_orientations: int = 6,\n",
    "                 min_wavelength: float = 3.0, mult: float = 2.1,\n",
    "                 sigma_on_f: float = 0.55):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_scales = num_scales\n",
    "        self.num_orientations = num_orientations\n",
    "        self.min_wavelength = min_wavelength\n",
    "        self.mult = mult\n",
    "        self.sigma_on_f = sigma_on_f\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        device = x.device\n",
    "        dtype = x.dtype\n",
    "        \n",
    "        # Create frequency grid\n",
    "        freq_y = torch.fft.fftfreq(H, device=device, dtype=dtype)\n",
    "        freq_x = torch.fft.fftfreq(W, device=device, dtype=dtype)\n",
    "        freq_y, freq_x = torch.meshgrid(freq_y, freq_x, indexing='ij')\n",
    "        \n",
    "        # Polar coordinates\n",
    "        radius = torch.sqrt(freq_x**2 + freq_y**2)\n",
    "        radius[0, 0] = 1  # Avoid log(0)\n",
    "        theta = torch.atan2(freq_y, freq_x)\n",
    "        \n",
    "        # FFT of input\n",
    "        x_fft = torch.fft.fft2(x)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for scale in range(self.num_scales):\n",
    "            wavelength = self.min_wavelength * (self.mult ** scale)\n",
    "            fo = 1.0 / wavelength  # Center frequency\n",
    "            \n",
    "            # Log-Gabor radial component\n",
    "            log_gabor_radial = torch.exp(\n",
    "                -(torch.log(radius / fo) ** 2) / (2 * math.log(self.sigma_on_f) ** 2)\n",
    "            )\n",
    "            log_gabor_radial[0, 0] = 0  # Zero DC\n",
    "            \n",
    "            for orient in range(self.num_orientations):\n",
    "                angle = orient * math.pi / self.num_orientations\n",
    "                \n",
    "                # Angular component\n",
    "                ds = torch.sin(theta - angle)\n",
    "                dc = torch.cos(theta - angle)\n",
    "                dtheta = torch.abs(torch.atan2(ds, dc))\n",
    "                \n",
    "                # Angular spread\n",
    "                angular_spread = torch.exp(\n",
    "                    -(dtheta ** 2) / (2 * (math.pi / self.num_orientations) ** 2)\n",
    "                )\n",
    "                \n",
    "                # Combined filter\n",
    "                log_gabor = log_gabor_radial * angular_spread\n",
    "                \n",
    "                # Apply filter\n",
    "                filtered = torch.fft.ifft2(x_fft * log_gabor.unsqueeze(0).unsqueeze(0))\n",
    "                outputs.append(filtered.abs())\n",
    "        \n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "class MonogenicSignal(nn.Module):\n",
    "\n",
    "    def __init__(self, epsilon: float = 1e-8):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.riesz = RieszTransform(epsilon=epsilon)\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> dict:\n",
    "        \n",
    "        # Get Riesz components\n",
    "        riesz_x, riesz_y = self.riesz(x)\n",
    "        \n",
    "        # Compute amplitude (local energy)\n",
    "        # A = sqrt(f^2 + h1^2 + h2^2)\n",
    "        amplitude = torch.sqrt(x**2 + riesz_x**2 + riesz_y**2 + self.epsilon)\n",
    "        \n",
    "        # Compute orientation\n",
    "        # theta = atan2(h2, h1)\n",
    "        orientation = torch.atan2(riesz_y, riesz_x + self.epsilon)\n",
    "        \n",
    "        # Compute phase\n",
    "        # phi = atan2(sqrt(h1^2 + h2^2), f)\n",
    "        riesz_magnitude = torch.sqrt(riesz_x**2 + riesz_y**2 + self.epsilon)\n",
    "        phase = torch.atan2(riesz_magnitude, x + self.epsilon)\n",
    "        \n",
    "        return {\n",
    "            'amplitude': amplitude,\n",
    "            'phase': phase,\n",
    "            'orientation': orientation,\n",
    "            'riesz_x': riesz_x,\n",
    "            'riesz_y': riesz_y\n",
    "        }\n",
    "\n",
    "class EnergyMap(nn.Module):\n",
    "\n",
    "    def __init__(self, normalize: bool = True, smoothing_sigma: float = 1.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.monogenic = MonogenicSignal()\n",
    "        self.normalize = normalize\n",
    "        self.smoothing_sigma = smoothing_sigma\n",
    "        \n",
    "        # Create Gaussian smoothing kernel\n",
    "        if smoothing_sigma > 0:\n",
    "            kernel_size = int(6 * smoothing_sigma) | 1  # Ensure odd\n",
    "            self.register_buffer('smooth_kernel', self._create_gaussian_kernel(\n",
    "                kernel_size, smoothing_sigma\n",
    "            ))\n",
    "        else:\n",
    "            self.smooth_kernel = None\n",
    "    \n",
    "    def _create_gaussian_kernel(self, kernel_size: int, sigma: float) -> torch.Tensor:\n",
    "        \n",
    "        x = torch.arange(kernel_size) - kernel_size // 2\n",
    "        x = x.float()\n",
    "        gaussian_1d = torch.exp(-x**2 / (2 * sigma**2))\n",
    "        gaussian_2d = gaussian_1d.unsqueeze(0) * gaussian_1d.unsqueeze(1)\n",
    "        gaussian_2d = gaussian_2d / gaussian_2d.sum()\n",
    "        return gaussian_2d.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, dict]:\n",
    "        \n",
    "        # Convert to grayscale if needed\n",
    "        if x.shape[1] > 1:\n",
    "            x = x.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        # Get monogenic decomposition\n",
    "        mono_out = self.monogenic(x)\n",
    "        \n",
    "        # Energy is the amplitude\n",
    "        energy = mono_out['amplitude']\n",
    "        \n",
    "        # Optional smoothing\n",
    "        if self.smooth_kernel is not None:\n",
    "            pad = self.smooth_kernel.shape[-1] // 2\n",
    "            energy = F.conv2d(energy, self.smooth_kernel, padding=pad)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        if self.normalize:\n",
    "            B = energy.shape[0]\n",
    "            energy_flat = energy.view(B, -1)\n",
    "            energy_min = energy_flat.min(dim=1, keepdim=True)[0].view(B, 1, 1, 1)\n",
    "            energy_max = energy_flat.max(dim=1, keepdim=True)[0].view(B, 1, 1, 1)\n",
    "            energy = (energy - energy_min) / (energy_max - energy_min + 1e-8)\n",
    "        \n",
    "        return energy, mono_out\n",
    "\n",
    "class BoundaryDetector(nn.Module):\n",
    "\n",
    "    def __init__(self, num_scales: int = 4, num_orientations: int = 6,\n",
    "                 noise_threshold: float = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.log_gabor = LogGaborFilter(num_scales, num_orientations)\n",
    "        self.num_scales = num_scales\n",
    "        self.num_orientations = num_orientations\n",
    "        self.noise_threshold = noise_threshold\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Get multi-scale responses\n",
    "        responses = self.log_gabor(x)  # (B, S*O, H, W)\n",
    "        \n",
    "        # Sum across orientations to get edge strength per scale\n",
    "        B, _, H, W = responses.shape\n",
    "        responses = responses.view(B, self.num_scales, self.num_orientations, H, W)\n",
    "        \n",
    "        # Max across orientations (strongest edge direction)\n",
    "        edge_strength = responses.max(dim=2)[0]  # (B, S, H, W)\n",
    "        \n",
    "        # Sum across scales\n",
    "        edge_strength = edge_strength.sum(dim=1, keepdim=True)  # (B, 1, H, W)\n",
    "        \n",
    "        # Normalize and threshold\n",
    "        edge_max = edge_strength.view(B, -1).max(dim=1)[0].view(B, 1, 1, 1)\n",
    "        edge_strength = edge_strength / (edge_max + 1e-8)\n",
    "        edge_strength = torch.clamp(edge_strength - self.noise_threshold, min=0)\n",
    "        edge_strength = edge_strength / (1 - self.noise_threshold + 1e-8)\n",
    "        \n",
    "        return edge_strength\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test Monogenic Signal processing\n",
    "    print(\"Testing Monogenic Signal Processing...\")\n",
    "    \n",
    "    # Create test image with edges\n",
    "    H, W = 128, 128\n",
    "    x = torch.zeros(1, 1, H, W)\n",
    "    x[:, :, 32:96, 32:96] = 1.0  # Square\n",
    "    \n",
    "    # Add some noise\n",
    "    x = x + 0.1 * torch.randn_like(x)\n",
    "    \n",
    "    # Test Energy Map\n",
    "    energy_extractor = EnergyMap(normalize=True)\n",
    "    energy, mono = energy_extractor(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Energy map shape: {energy.shape}\")\n",
    "    print(f\"Energy range: [{energy.min():.3f}, {energy.max():.3f}]\")\n",
    "    print(f\"Monogenic components: {list(mono.keys())}\")\n",
    "    \n",
    "    # Test Boundary Detector\n",
    "    boundary_detector = BoundaryDetector()\n",
    "    boundaries = boundary_detector(x)\n",
    "    \n",
    "    print(f\"Boundary map shape: {boundaries.shape}\")\n",
    "    print(f\"Boundary range: [{boundaries.min():.3f}, {boundaries.max():.3f}]\")\n",
    "    \n",
    "    print(\"\\nâœ“ All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd15abcd",
   "metadata": {},
   "source": [
    "### 2.4 Gabor Implicit Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f2b266c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Gabor Implicit Modules...\n",
      "\n",
      "[1] Testing GaborBasis...\n",
      "Input coords: torch.Size([4, 100, 2])\n",
      "Gabor encoded: torch.Size([4, 100, 64])\n",
      "\n",
      "[2] Testing GaborNet...\n",
      "GaborNet output: torch.Size([4, 100, 3])\n",
      "\n",
      "[3] Testing ImplicitSegmentationHead...\n",
      "Feature map: torch.Size([2, 64, 32, 32])\n",
      "Segmentation output (grid): torch.Size([2, 3, 128, 128])\n",
      "Segmentation output (points): torch.Size([2, 500, 3])\n",
      "\n",
      "âœ“ All tests passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "class GaborBasis(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int = 2, num_frequencies: int = 64,\n",
    "                 sigma_range: Tuple[float, float] = (0.1, 2.0),\n",
    "                 freq_range: Tuple[float, float] = (1.0, 10.0),\n",
    "                 learnable: bool = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.output_dim = num_frequencies * 2  # sin and cos components\n",
    "        \n",
    "        # Initialize frequencies uniformly in log space\n",
    "        log_freqs = torch.linspace(\n",
    "            math.log(freq_range[0]), \n",
    "            math.log(freq_range[1]), \n",
    "            num_frequencies\n",
    "        )\n",
    "        freqs = torch.exp(log_freqs)\n",
    "        \n",
    "        # Initialize sigmas (Gaussian envelope widths)\n",
    "        sigmas = torch.linspace(sigma_range[0], sigma_range[1], num_frequencies)\n",
    "        \n",
    "        # Random orientations for 2D\n",
    "        orientations = torch.rand(num_frequencies) * 2 * math.pi\n",
    "        \n",
    "        # Random phases\n",
    "        phases = torch.rand(num_frequencies) * 2 * math.pi\n",
    "        \n",
    "        # Create direction vectors from orientations\n",
    "        directions = torch.stack([\n",
    "            torch.cos(orientations),\n",
    "            torch.sin(orientations)\n",
    "        ], dim=-1)  # (num_freq, 2)\n",
    "        \n",
    "        if learnable:\n",
    "            self.freqs = nn.Parameter(freqs)\n",
    "            self.sigmas = nn.Parameter(sigmas)\n",
    "            self.directions = nn.Parameter(directions)\n",
    "            self.phases = nn.Parameter(phases)\n",
    "        else:\n",
    "            self.register_buffer('freqs', freqs)\n",
    "            self.register_buffer('sigmas', sigmas)\n",
    "            self.register_buffer('directions', directions)\n",
    "            self.register_buffer('phases', phases)\n",
    "    \n",
    "    def forward(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Normalize directions\n",
    "        directions = F.normalize(self.directions, dim=-1)  # (num_freq, 2)\n",
    "        \n",
    "        # Project coordinates onto directions\n",
    "        # coords: (..., 2), directions: (num_freq, 2)\n",
    "        proj = torch.matmul(coords, directions.T)  # (..., num_freq)\n",
    "        \n",
    "        # Compute Gaussian envelope\n",
    "        # exp(-projÂ² / (2ÏƒÂ²))\n",
    "        sigmas = torch.abs(self.sigmas) + 0.01  # Ensure positive\n",
    "        gaussian = torch.exp(-proj**2 / (2 * sigmas**2 + 1e-8))\n",
    "        \n",
    "        # Compute oscillatory component\n",
    "        # cos(2Ï€fÂ·proj + Ï†), sin(2Ï€fÂ·proj + Ï†)\n",
    "        freqs = torch.abs(self.freqs) + 0.1  # Ensure positive\n",
    "        arg = 2 * math.pi * freqs * proj + self.phases\n",
    "        \n",
    "        cos_comp = gaussian * torch.cos(arg)\n",
    "        sin_comp = gaussian * torch.sin(arg)\n",
    "        \n",
    "        # Concatenate sin and cos\n",
    "        gabor_features = torch.cat([cos_comp, sin_comp], dim=-1)\n",
    "        \n",
    "        return gabor_features\n",
    "\n",
    "class FourierFeatures(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int = 2, num_frequencies: int = 64,\n",
    "                 scale: float = 10.0, learnable: bool = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.output_dim = num_frequencies * 2\n",
    "        \n",
    "        # Random frequency matrix\n",
    "        B = torch.randn(input_dim, num_frequencies) * scale\n",
    "        \n",
    "        if learnable:\n",
    "            self.B = nn.Parameter(B)\n",
    "        else:\n",
    "            self.register_buffer('B', B)\n",
    "    \n",
    "    def forward(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Project: coords @ B\n",
    "        proj = 2 * math.pi * torch.matmul(coords, self.B)  # (..., num_freq)\n",
    "        \n",
    "        # Sin and cos\n",
    "        return torch.cat([torch.cos(proj), torch.sin(proj)], dim=-1)\n",
    "\n",
    "class SIRENLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, \n",
    "                 omega_0: float = 30.0, is_first: bool = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.omega_0 = omega_0\n",
    "        self.is_first = is_first\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if self.is_first:\n",
    "                # First layer: uniform in [-1/n, 1/n]\n",
    "                self.linear.weight.uniform_(-1 / self.in_features, \n",
    "                                            1 / self.in_features)\n",
    "            else:\n",
    "                # Other layers: uniform in [-sqrt(6/n)/Ï‰â‚€, sqrt(6/n)/Ï‰â‚€]\n",
    "                bound = math.sqrt(6 / self.in_features) / self.omega_0\n",
    "                self.linear.weight.uniform_(-bound, bound)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sin(self.omega_0 * self.linear(x))\n",
    "\n",
    "class GaborNet(nn.Module):\n",
    "\n",
    "    def __init__(self, coord_dim: int = 2, feature_dim: int = 256,\n",
    "                 hidden_dim: int = 256, output_dim: int = 1,\n",
    "                 num_layers: int = 4, num_frequencies: int = 64,\n",
    "                 use_gabor: bool = True, omega_0: float = 30.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Coordinate encoding\n",
    "        if use_gabor:\n",
    "            self.coord_encoder = GaborBasis(\n",
    "                input_dim=coord_dim,\n",
    "                num_frequencies=num_frequencies,\n",
    "                learnable=True\n",
    "            )\n",
    "        else:\n",
    "            self.coord_encoder = FourierFeatures(\n",
    "                input_dim=coord_dim,\n",
    "                num_frequencies=num_frequencies,\n",
    "                learnable=False\n",
    "            )\n",
    "        \n",
    "        coord_encoded_dim = self.coord_encoder.output_dim\n",
    "        \n",
    "        # Input dimension: encoded coords + features\n",
    "        input_dim = coord_encoded_dim + feature_dim\n",
    "        \n",
    "        # Build SIREN network\n",
    "        layers = []\n",
    "        \n",
    "        # First layer\n",
    "        layers.append(SIRENLayer(input_dim, hidden_dim, omega_0, is_first=True))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(SIRENLayer(hidden_dim, hidden_dim, omega_0, is_first=False))\n",
    "        \n",
    "        # Final layer (linear, no sine activation)\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, coords: torch.Tensor, features: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Encode coordinates\n",
    "        coord_encoded = self.coord_encoder(coords)  # (B, N, coord_encoded_dim)\n",
    "        \n",
    "        # Concatenate with features\n",
    "        x = torch.cat([coord_encoded, features], dim=-1)  # (B, N, input_dim)\n",
    "        \n",
    "        # Pass through network\n",
    "        output = self.network(x)  # (B, N, output_dim)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class ImplicitSegmentationHead(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_channels: int = 64, num_classes: int = 2,\n",
    "                 hidden_dim: int = 256, num_layers: int = 4,\n",
    "                 num_frequencies: int = 64, use_gabor: bool = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_channels = feature_channels\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Feature projector (reduce channel dimension)\n",
    "        self.feature_proj = nn.Sequential(\n",
    "            nn.Conv2d(feature_channels, hidden_dim, kernel_size=1),\n",
    "            nn.GroupNorm(32, hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Implicit decoder\n",
    "        self.implicit_decoder = GaborNet(\n",
    "            coord_dim=2,\n",
    "            feature_dim=hidden_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=num_classes,\n",
    "            num_layers=num_layers,\n",
    "            num_frequencies=num_frequencies,\n",
    "            use_gabor=use_gabor\n",
    "        )\n",
    "    \n",
    "    def sample_features(self, feature_map: torch.Tensor, \n",
    "                       coords: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        B, C, H, W = feature_map.shape\n",
    "        N = coords.shape[1]\n",
    "        \n",
    "        # Reshape coords for grid_sample: (B, N, 1, 2) -> (B, 1, N, 2)\n",
    "        # grid_sample expects (B, H, W, 2) where last dim is (x, y)\n",
    "        grid = coords.view(B, 1, N, 2)\n",
    "        \n",
    "        # Sample using bilinear interpolation\n",
    "        # feature_map: (B, C, H, W), grid: (B, 1, N, 2)\n",
    "        # output: (B, C, 1, N)\n",
    "        sampled = F.grid_sample(\n",
    "            feature_map, grid,\n",
    "            mode='bilinear',\n",
    "            padding_mode='border',\n",
    "            align_corners=True\n",
    "        )\n",
    "        \n",
    "        # Reshape: (B, C, 1, N) -> (B, N, C)\n",
    "        sampled = sampled.squeeze(2).permute(0, 2, 1)\n",
    "        \n",
    "        return sampled\n",
    "    \n",
    "    def forward(self, feature_map: torch.Tensor, \n",
    "                coords: Optional[torch.Tensor] = None,\n",
    "                output_size: Optional[Tuple[int, int]] = None) -> torch.Tensor:\n",
    "        \n",
    "        B, C, H_feat, W_feat = feature_map.shape\n",
    "        device = feature_map.device\n",
    "        \n",
    "        # Project features\n",
    "        feature_map = self.feature_proj(feature_map)  # (B, hidden_dim, H, W)\n",
    "        \n",
    "        # Generate coordinates if not provided\n",
    "        if coords is None:\n",
    "            if output_size is None:\n",
    "                output_size = (H_feat * 4, W_feat * 4)\n",
    "            \n",
    "            H_out, W_out = output_size\n",
    "            \n",
    "            # Create normalized coordinate grid [-1, 1]\n",
    "            y = torch.linspace(-1, 1, H_out, device=device)\n",
    "            x = torch.linspace(-1, 1, W_out, device=device)\n",
    "            yy, xx = torch.meshgrid(y, x, indexing='ij')\n",
    "            coords = torch.stack([xx, yy], dim=-1)  # (H_out, W_out, 2)\n",
    "            coords = coords.view(1, -1, 2).expand(B, -1, -1)  # (B, H*W, 2)\n",
    "            \n",
    "            reshape_output = True\n",
    "        else:\n",
    "            reshape_output = False\n",
    "            H_out, W_out = None, None\n",
    "        \n",
    "        # Sample features at coordinates\n",
    "        features = self.sample_features(feature_map, coords)  # (B, N, hidden_dim)\n",
    "        \n",
    "        # Implicit decoding\n",
    "        logits = self.implicit_decoder(coords, features)  # (B, N, num_classes)\n",
    "        \n",
    "        # Reshape to image if using grid\n",
    "        if reshape_output:\n",
    "            logits = logits.view(B, H_out, W_out, self.num_classes)\n",
    "            logits = logits.permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing Gabor Implicit Modules...\")\n",
    "    \n",
    "    # Test Gabor Basis\n",
    "    print(\"\\n[1] Testing GaborBasis...\")\n",
    "    gabor = GaborBasis(input_dim=2, num_frequencies=32)\n",
    "    coords = torch.randn(4, 100, 2)  # (B, N, 2)\n",
    "    encoded = gabor(coords)\n",
    "    print(f\"Input coords: {coords.shape}\")\n",
    "    print(f\"Gabor encoded: {encoded.shape}\")\n",
    "    \n",
    "    # Test GaborNet\n",
    "    print(\"\\n[2] Testing GaborNet...\")\n",
    "    net = GaborNet(coord_dim=2, feature_dim=64, hidden_dim=128, \n",
    "                   output_dim=3, num_layers=3, num_frequencies=32)\n",
    "    features = torch.randn(4, 100, 64)\n",
    "    output = net(coords, features)\n",
    "    print(f\"GaborNet output: {output.shape}\")\n",
    "    \n",
    "    # Test ImplicitSegmentationHead\n",
    "    print(\"\\n[3] Testing ImplicitSegmentationHead...\")\n",
    "    head = ImplicitSegmentationHead(\n",
    "        feature_channels=64, num_classes=3,\n",
    "        hidden_dim=128, num_layers=3, num_frequencies=32\n",
    "    )\n",
    "    feature_map = torch.randn(2, 64, 32, 32)\n",
    "    \n",
    "    # Test with automatic grid\n",
    "    seg_output = head(feature_map, output_size=(128, 128))\n",
    "    print(f\"Feature map: {feature_map.shape}\")\n",
    "    print(f\"Segmentation output (grid): {seg_output.shape}\")\n",
    "    \n",
    "    # Test with custom coordinates\n",
    "    custom_coords = torch.rand(2, 500, 2) * 2 - 1  # Random points in [-1, 1]\n",
    "    seg_points = head(feature_map, coords=custom_coords)\n",
    "    print(f\"Segmentation output (points): {seg_points.shape}\")\n",
    "    \n",
    "    print(\"\\nâœ“ All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b14d5fe",
   "metadata": {},
   "source": [
    "### 2.5 Spectral Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1093c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 1, 256, 256])\n",
      "Output shape: torch.Size([2, 3, 256, 256])\n",
      "Total parameters: 10,312,523\n",
      "Trainable parameters: 10,312,523\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, List\n",
    "\n",
    "class SpectralVSSBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, height: int, width: int,\n",
    "                 depth: int = 2, expansion_ratio: float = 2.0,\n",
    "                 threshold: float = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # Branch A: Spatial path (VSS Blocks)\n",
    "        self.vss_blocks = MambaBlockStack(\n",
    "            channels, depth=depth, \n",
    "            expansion_ratio=expansion_ratio, \n",
    "            scan_dim=min(64, channels)\n",
    "        )\n",
    "        \n",
    "        # Branch B: Spectral path (FFT-based filtering)\n",
    "        self.spectral_gate = SpectralGating(\n",
    "            channels, height, width, \n",
    "            threshold=threshold, \n",
    "            complex_init=\"kaiming\"\n",
    "        )\n",
    "        \n",
    "        # Fusion layer (learnable weighting)\n",
    "        self.fusion_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Branch A: Spatial context (VSS)\n",
    "        spatial_out = self.vss_blocks(x)\n",
    "        \n",
    "        # Branch B: Frequency filtering (Spectral)\n",
    "        spectral_out = self.spectral_gate(x)\n",
    "        \n",
    "        # Learnable fusion with sigmoid weight\n",
    "        weight = torch.sigmoid(self.fusion_weight)\n",
    "        output = weight * spatial_out + (1 - weight) * spectral_out\n",
    "        \n",
    "        return output\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int = 3, out_channels: int = 64, \n",
    "                 patch_size: int = 4):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels,\n",
    "            kernel_size=patch_size, stride=patch_size, bias=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(out_channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, out_channels: Optional[int] = None):\n",
    "        \n",
    "        super().__init__()\n",
    "        out_channels = out_channels or channels * 2\n",
    "        self.conv = nn.Conv2d(channels, out_channels, kernel_size=2, \n",
    "                              stride=2, bias=True)\n",
    "        self.norm = nn.LayerNorm(out_channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "class PatchExpanding(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, out_channels: Optional[int] = None):\n",
    "        \n",
    "        super().__init__()\n",
    "        out_channels = out_channels or channels // 2\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', \n",
    "                                    align_corners=True)\n",
    "        self.conv = nn.Conv2d(channels, out_channels, kernel_size=1, bias=True)\n",
    "        self.norm = nn.LayerNorm(out_channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv(x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "class SpectralVMUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int = 1, out_channels: int = 3,\n",
    "                 img_size: int = 256, base_channels: int = 64,\n",
    "                 num_stages: int = 4, depth: int = 2):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.img_size = img_size\n",
    "        self.base_channels = base_channels\n",
    "        self.num_stages = num_stages\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(in_channels, base_channels, patch_size=4)\n",
    "        initial_size = img_size // 4\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        self.downsample_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_stages):\n",
    "            in_ch = base_channels * (2 ** i)\n",
    "            out_ch = in_ch\n",
    "            h = w = initial_size // (2 ** i)\n",
    "            \n",
    "            # SpectralVSSBlock\n",
    "            block = SpectralVSSBlock(\n",
    "                in_ch, h, w, depth=depth, expansion_ratio=2.0, threshold=0.1\n",
    "            )\n",
    "            self.encoder_blocks.append(block)\n",
    "            \n",
    "            # Downsampling (except after last encoder block)\n",
    "            if i < num_stages - 1:\n",
    "                down = PatchMerging(in_ch, in_ch * 2)\n",
    "                self.downsample_layers.append(down)\n",
    "        \n",
    "        # Bottleneck - uses the last encoder's output channels\n",
    "        # After num_stages-1 downsamplings, channels = base_channels * 2^(num_stages-1)\n",
    "        bottleneck_ch = base_channels * (2 ** (num_stages - 1))\n",
    "        bottleneck_h = bottleneck_w = initial_size // (2 ** (num_stages - 1))\n",
    "        self.bottleneck = SpectralVSSBlock(\n",
    "            bottleneck_ch, bottleneck_h, bottleneck_w,\n",
    "            depth=depth + 1, expansion_ratio=2.0, threshold=0.1\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        # We have num_stages - 1 decoder stages (matching skip connections)\n",
    "        # Each decoder stage: upsample -> concat with skip -> fusion -> SpectralVSSBlock\n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "        self.upsample_layers = nn.ModuleList()\n",
    "        \n",
    "        num_decoder_stages = num_stages - 1\n",
    "        \n",
    "        for i in range(num_decoder_stages):\n",
    "            # Going from deepest to shallowest\n",
    "            # i=0: from bottleneck (8x8, 512ch) -> upsample to (16x16, 256ch)\n",
    "            # i=1: from 16x16, 256ch -> upsample to (32x32, 128ch)\n",
    "            # i=2: from 32x32, 128ch -> upsample to (64x64, 64ch)\n",
    "            \n",
    "            # Input channels: for i=0, it's bottleneck_ch; else from previous decoder output\n",
    "            if i == 0:\n",
    "                in_ch = bottleneck_ch  # 512 for default\n",
    "            else:\n",
    "                in_ch = base_channels * (2 ** (num_stages - 1 - i))\n",
    "            \n",
    "            # Output channels after upsampling\n",
    "            out_ch = base_channels * (2 ** (num_stages - 2 - i))\n",
    "            \n",
    "            # Upsampling layer\n",
    "            up = PatchExpanding(in_ch, out_ch)\n",
    "            self.upsample_layers.append(up)\n",
    "            \n",
    "            # Skip connection comes from encoder at level (num_decoder_stages - 1 - i)\n",
    "            # which has same spatial size after upsampling\n",
    "            skip_ch = out_ch  # Skip has same channels as upsampled output\n",
    "            \n",
    "            # Spatial size at this level\n",
    "            h = w = initial_size // (2 ** (num_stages - 2 - i))\n",
    "            \n",
    "            # Fusion: concatenate upsampled + skip, then reduce channels\n",
    "            fused_ch = out_ch + skip_ch  # After concatenation\n",
    "            fusion = nn.Sequential(\n",
    "                nn.Conv2d(fused_ch, out_ch, kernel_size=1, bias=True),\n",
    "                nn.GroupNorm(num_groups=min(32, out_ch), num_channels=out_ch, eps=1e-6)\n",
    "            )\n",
    "            self.decoder_blocks.append(fusion)\n",
    "            \n",
    "            # SpectralVSSBlock after fusion\n",
    "            vss = SpectralVSSBlock(\n",
    "                out_ch, h, w, depth=depth, expansion_ratio=2.0, threshold=0.1\n",
    "            )\n",
    "            self.decoder_blocks.append(vss)\n",
    "        \n",
    "        # Segmentation head\n",
    "        self.seg_head = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels // 2, kernel_size=3, \n",
    "                      padding=1, bias=True),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=base_channels // 2, eps=1e-6),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(base_channels // 2, out_channels, kernel_size=1, bias=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Encoder path with skip connections storage\n",
    "        # Skip connections are saved BEFORE downsampling\n",
    "        skips = []\n",
    "        for i in range(self.num_stages):\n",
    "            x = self.encoder_blocks[i](x)\n",
    "            # Save skip connection before downsampling\n",
    "            if i < self.num_stages - 1:\n",
    "                skips.append(x)\n",
    "                x = self.downsample_layers[i](x)\n",
    "        \n",
    "        # The last encoder output goes to bottleneck (no skip for this level)\n",
    "        # skips now contains: [stage0_out, stage1_out, stage2_out] for 4 stages\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Decoder path with skip connections\n",
    "        # Decoder stages: num_stages - 1 (since last encoder has no skip)\n",
    "        num_decoder_stages = self.num_stages - 1\n",
    "        \n",
    "        for i in range(num_decoder_stages):\n",
    "            # Upsample\n",
    "            x = self.upsample_layers[i](x)\n",
    "            \n",
    "            # Concatenate skip connection (in reverse order)\n",
    "            # For i=0: skip from encoder stage num_stages-2 (last skip)\n",
    "            # For i=1: skip from encoder stage num_stages-3\n",
    "            skip_idx = num_decoder_stages - 1 - i\n",
    "            skip = skips[skip_idx]\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            \n",
    "            # Fusion and processing\n",
    "            x = self.decoder_blocks[2 * i](x)  # Fusion conv\n",
    "            x = self.decoder_blocks[2 * i + 1](x)  # SpectralVSSBlock\n",
    "        \n",
    "        # Segmentation head\n",
    "        output = self.seg_head(x)\n",
    "        \n",
    "        # Upsample to original resolution (since patch embedding uses stride 4)\n",
    "        output = F.interpolate(output, size=(self.img_size, self.img_size),\n",
    "                               mode='bilinear', align_corners=True)\n",
    "        \n",
    "        return output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the full architecture\n",
    "    batch_size = 2\n",
    "    in_channels = 1\n",
    "    out_channels = 3  # Binary segmentation + background\n",
    "    img_size = 256\n",
    "    \n",
    "    model = SpectralVMUNet(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        img_size=img_size,\n",
    "        base_channels=64,\n",
    "        num_stages=4,\n",
    "        depth=2\n",
    "    )\n",
    "    \n",
    "    # Create dummy input\n",
    "    x = torch.randn(batch_size, in_channels, img_size, img_size)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72355acc",
   "metadata": {},
   "source": [
    "### 2.6 EGM-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b729a20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing EGM-Net (Energy-Gated Gabor Mamba Network)\n",
      "============================================================\n",
      "\n",
      "[1] Testing EGM-Net Full...\n",
      "Input: torch.Size([2, 1, 256, 256])\n",
      "Output: torch.Size([2, 3, 256, 256])\n",
      "Coarse: torch.Size([2, 3, 256, 256])\n",
      "Fine: torch.Size([2, 3, 256, 256])\n",
      "Energy: torch.Size([2, 1, 256, 256])\n",
      "\n",
      "Total parameters: 9,133,192\n",
      "Trainable parameters: 9,133,192\n",
      "\n",
      "[2] Testing Point Query (Resolution-Free)...\n",
      "Query coords: torch.Size([2, 1000, 2])\n",
      "Point output: torch.Size([2, 1000, 3])\n",
      "\n",
      "[3] Testing EGM-Net Lite...\n",
      "Lite model parameters: 635,272\n",
      "Lite output: torch.Size([2, 3, 256, 256])\n",
      "\n",
      "============================================================\n",
      "âœ“ All tests passed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple, Dict\n",
    "import math\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int = 1, embed_dim: int = 64, patch_size: int = 4):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, \n",
    "                              stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.proj(x)  # (B, C, H, W)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "class DownsampleBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.norm = nn.GroupNorm(32, out_channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.norm(self.conv(x))\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.norm = nn.GroupNorm(min(32, out_channels), out_channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.norm(self.conv(self.up(x)))\n",
    "\n",
    "class MambaEncoderStage(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, depth: int = 2, spatial_size: int = 64):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VSSBlock(channels, scan_dim=min(64, channels))\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "class CoarseBranch(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, num_classes: int, num_stages: int = 3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.upsample_layers = nn.ModuleList()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        \n",
    "        channels = in_channels\n",
    "        for i in range(num_stages):\n",
    "            out_ch = max(channels // 2, 64)\n",
    "            self.upsample_layers.append(UpsampleBlock(channels, out_ch))\n",
    "            self.conv_layers.append(nn.Sequential(\n",
    "                nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "                nn.GroupNorm(min(32, out_ch), out_ch),\n",
    "                nn.GELU()\n",
    "            ))\n",
    "            channels = out_ch\n",
    "        \n",
    "        self.head = nn.Conv2d(channels, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for up, conv in zip(self.upsample_layers, self.conv_layers):\n",
    "            x = up(x)\n",
    "            x = conv(x)\n",
    "        return self.head(x)\n",
    "\n",
    "class EnergyGatedFusion(nn.Module):\n",
    "\n",
    "    def __init__(self, temperature: float = 1.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.gate_scale = nn.Parameter(torch.ones(1))\n",
    "        self.gate_bias = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, coarse: torch.Tensor, fine: torch.Tensor, \n",
    "                energy: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Resize energy to match prediction size\n",
    "        if energy.shape[-2:] != coarse.shape[-2:]:\n",
    "            energy = F.interpolate(energy, size=coarse.shape[-2:], \n",
    "                                   mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # Apply learnable scaling and temperature\n",
    "        gate = torch.sigmoid((energy * self.gate_scale + self.gate_bias) / self.temperature)\n",
    "        \n",
    "        # Blend: high energy â†’ use fine, low energy â†’ use coarse\n",
    "        output = coarse + gate * (fine - coarse)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class FineBranch(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_channels: int, num_classes: int,\n",
    "                 hidden_dim: int = 256, num_layers: int = 4,\n",
    "                 num_frequencies: int = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.implicit_head = ImplicitSegmentationHead(\n",
    "            feature_channels=feature_channels,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_frequencies=num_frequencies,\n",
    "            use_gabor=True  # Use Gabor instead of Fourier\n",
    "        )\n",
    "    \n",
    "    def forward(self, features: torch.Tensor, \n",
    "                coords: Optional[torch.Tensor] = None,\n",
    "                output_size: Optional[Tuple[int, int]] = None) -> torch.Tensor:\n",
    "        \n",
    "        return self.implicit_head(features, coords, output_size)\n",
    "\n",
    "class EGMNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int = 1, num_classes: int = 2,\n",
    "                 img_size: int = 256, base_channels: int = 64,\n",
    "                 num_stages: int = 4, encoder_depth: int = 2,\n",
    "                 implicit_hidden: int = 256, implicit_layers: int = 4,\n",
    "                 num_frequencies: int = 64):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        self.num_stages = num_stages\n",
    "        \n",
    "        # 1. Monogenic Energy Extractor (fixed, non-trainable)\n",
    "        self.energy_extractor = EnergyMap(normalize=True, smoothing_sigma=1.0)\n",
    "        \n",
    "        # 2. Patch Embedding\n",
    "        self.patch_embed = PatchEmbedding(in_channels, base_channels, patch_size=4)\n",
    "        feat_size = img_size // 4\n",
    "        \n",
    "        # 3. Mamba Encoder\n",
    "        self.encoder_stages = nn.ModuleList()\n",
    "        self.downsample_layers = nn.ModuleList()\n",
    "        \n",
    "        channels = base_channels\n",
    "        for i in range(num_stages):\n",
    "            self.encoder_stages.append(\n",
    "                MambaEncoderStage(channels, depth=encoder_depth, spatial_size=feat_size)\n",
    "            )\n",
    "            if i < num_stages - 1:\n",
    "                self.downsample_layers.append(\n",
    "                    DownsampleBlock(channels, channels * 2)\n",
    "                )\n",
    "                channels *= 2\n",
    "                feat_size //= 2\n",
    "        \n",
    "        # Store final encoder channels\n",
    "        self.encoder_channels = channels\n",
    "        \n",
    "        # 4. Bottleneck\n",
    "        self.bottleneck = MambaEncoderStage(\n",
    "            channels, depth=encoder_depth + 1, spatial_size=feat_size\n",
    "        )\n",
    "        \n",
    "        # 5. Coarse Branch (standard decoder)\n",
    "        self.coarse_branch = CoarseBranch(\n",
    "            in_channels=channels,\n",
    "            num_classes=num_classes,\n",
    "            num_stages=num_stages - 1\n",
    "        )\n",
    "        \n",
    "        # 6. Fine Branch (Gabor implicit decoder)\n",
    "        self.fine_branch = FineBranch(\n",
    "            feature_channels=channels,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dim=implicit_hidden,\n",
    "            num_layers=implicit_layers,\n",
    "            num_frequencies=num_frequencies\n",
    "        )\n",
    "        \n",
    "        # 7. Energy-Gated Fusion\n",
    "        self.fusion = EnergyGatedFusion(temperature=1.0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, \n",
    "                output_size: Optional[Tuple[int, int]] = None) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        if output_size is None:\n",
    "            output_size = (H, W)\n",
    "        \n",
    "        # 1. Extract energy map (detached, no gradients for physics module)\n",
    "        with torch.no_grad():\n",
    "            # Convert to grayscale if needed\n",
    "            x_gray = x.mean(dim=1, keepdim=True) if C > 1 else x\n",
    "            energy, mono_out = self.energy_extractor(x_gray)\n",
    "        \n",
    "        # 2. Patch embedding\n",
    "        features = self.patch_embed(x)\n",
    "        \n",
    "        # 3. Encoder (Mamba stages)\n",
    "        encoder_features = []\n",
    "        for i, stage in enumerate(self.encoder_stages):\n",
    "            features = stage(features)\n",
    "            encoder_features.append(features)\n",
    "            if i < len(self.downsample_layers):\n",
    "                features = self.downsample_layers[i](features)\n",
    "        \n",
    "        # 4. Bottleneck\n",
    "        features = self.bottleneck(features)\n",
    "        \n",
    "        # 5. Coarse branch\n",
    "        coarse = self.coarse_branch(features)\n",
    "        coarse = F.interpolate(coarse, size=output_size, \n",
    "                               mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # 6. Fine branch (implicit decoder)\n",
    "        fine = self.fine_branch(features, output_size=output_size)\n",
    "        \n",
    "        # 7. Energy-gated fusion\n",
    "        output = self.fusion(coarse, fine, energy)\n",
    "        \n",
    "        return {\n",
    "            'output': output,\n",
    "            'coarse': coarse,\n",
    "            'fine': fine,\n",
    "            'energy': energy\n",
    "        }\n",
    "    \n",
    "    def inference(self, x: torch.Tensor, \n",
    "                  output_size: Optional[Tuple[int, int]] = None) -> torch.Tensor:\n",
    "        \n",
    "        return self.forward(x, output_size)['output']\n",
    "    \n",
    "    def query_points(self, x: torch.Tensor, \n",
    "                     coords: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Encode image\n",
    "        features = self.patch_embed(x)\n",
    "        for i, stage in enumerate(self.encoder_stages):\n",
    "            features = stage(features)\n",
    "            if i < len(self.downsample_layers):\n",
    "                features = self.downsample_layers[i](features)\n",
    "        features = self.bottleneck(features)\n",
    "        \n",
    "        # Query fine branch at coordinates\n",
    "        fine_points = self.fine_branch.implicit_head(features, coords=coords)\n",
    "        \n",
    "        return fine_points\n",
    "\n",
    "class EGMNetLite(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int = 1, num_classes: int = 2,\n",
    "                 img_size: int = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = EGMNet(\n",
    "            in_channels=in_channels,\n",
    "            num_classes=num_classes,\n",
    "            img_size=img_size,\n",
    "            base_channels=32,  # Reduced from 64\n",
    "            num_stages=3,      # Reduced from 4\n",
    "            encoder_depth=1,   # Reduced from 2\n",
    "            implicit_hidden=128,  # Reduced from 256\n",
    "            implicit_layers=3,    # Reduced from 4\n",
    "            num_frequencies=32    # Reduced from 64\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, output_size=None):\n",
    "        return self.model(x, output_size)\n",
    "    \n",
    "    def inference(self, x, output_size=None):\n",
    "        return self.model.inference(x, output_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Testing EGM-Net (Energy-Gated Gabor Mamba Network)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test full model\n",
    "    print(\"\\n[1] Testing EGM-Net Full...\")\n",
    "    model = EGMNet(\n",
    "        in_channels=1,\n",
    "        num_classes=3,\n",
    "        img_size=256,\n",
    "        base_channels=64,\n",
    "        num_stages=4,\n",
    "        encoder_depth=2\n",
    "    )\n",
    "    \n",
    "    x = torch.randn(2, 1, 256, 256)\n",
    "    outputs = model(x)\n",
    "    \n",
    "    print(f\"Input: {x.shape}\")\n",
    "    print(f\"Output: {outputs['output'].shape}\")\n",
    "    print(f\"Coarse: {outputs['coarse'].shape}\")\n",
    "    print(f\"Fine: {outputs['fine'].shape}\")\n",
    "    print(f\"Energy: {outputs['energy'].shape}\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Test point query (resolution-free inference)\n",
    "    print(\"\\n[2] Testing Point Query (Resolution-Free)...\")\n",
    "    coords = torch.rand(2, 1000, 2) * 2 - 1  # Random points in [-1, 1]\n",
    "    point_output = model.query_points(x, coords)\n",
    "    print(f\"Query coords: {coords.shape}\")\n",
    "    print(f\"Point output: {point_output.shape}\")\n",
    "    \n",
    "    # Test lite model\n",
    "    print(\"\\n[3] Testing EGM-Net Lite...\")\n",
    "    lite_model = EGMNetLite(in_channels=1, num_classes=3, img_size=256)\n",
    "    lite_outputs = lite_model(x)\n",
    "    \n",
    "    lite_params = sum(p.numel() for p in lite_model.parameters())\n",
    "    print(f\"Lite model parameters: {lite_params:,}\")\n",
    "    print(f\"Lite output: {lite_outputs['output'].shape}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"âœ“ All tests passed!\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39339369",
   "metadata": {},
   "source": [
    "### 2.7 Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c6472c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: 1.7057\n",
      "  dice: 0.6659\n",
      "  focal: 0.9038\n",
      "  freq: 1.3602\n",
      "  total: 1.7057\n",
      "\n",
      "Boundary Loss: 2.7721\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, smooth: float = 1e-5, reduction: str = \"mean\"):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        pred = torch.softmax(pred, dim=1)\n",
    "        \n",
    "        # Ensure target has same shape as pred for multi-class\n",
    "        if target.ndim == 3:  # (B, H, W) -> convert to one-hot\n",
    "            target = F.one_hot(target.long(), num_classes=pred.shape[1])\n",
    "            target = target.permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        # Flatten spatial dimensions\n",
    "        pred = pred.view(pred.shape[0], pred.shape[1], -1)\n",
    "        target = target.view(target.shape[0], target.shape[1], -1)\n",
    "        \n",
    "        # Compute Dice score\n",
    "        intersection = torch.sum(pred * target, dim=2)\n",
    "        union = torch.sum(pred, dim=2) + torch.sum(target, dim=2)\n",
    "        \n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        # Return loss (1 - Dice)\n",
    "        loss = 1.0 - dice\n",
    "        \n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha: float = 0.25, gamma: float = 2.0,\n",
    "                 reduction: str = \"mean\"):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Get class probabilities\n",
    "        p = torch.softmax(pred, dim=1)\n",
    "        \n",
    "        # Get class log probabilities\n",
    "        ce = F.cross_entropy(pred, target.long(), reduction='none')\n",
    "        \n",
    "        # Get probability of true class\n",
    "        p_t = torch.gather(p, 1, target.long().unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute focal loss\n",
    "        focal_weight = (1.0 - p_t) ** self.gamma\n",
    "        focal_loss = focal_weight * ce\n",
    "        \n",
    "        if self.reduction == \"mean\":\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class FrequencyLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, weight: float = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Ensure both have batch and channel dimensions\n",
    "        if pred.ndim == 3:\n",
    "            pred = pred.unsqueeze(1)\n",
    "        if target.ndim == 3:\n",
    "            target = target.unsqueeze(1)\n",
    "        \n",
    "        # Flatten to single channel for FFT comparison\n",
    "        if pred.shape[1] > 1:\n",
    "            # For multi-channel, convert to grayscale by averaging\n",
    "            pred = pred.mean(dim=1, keepdim=True)\n",
    "        if target.shape[1] > 1:\n",
    "            target = target.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        # Apply FFT to convert to frequency domain\n",
    "        pred_freq = torch.fft.rfft2(pred, dim=(-2, -1), norm=\"ortho\")\n",
    "        target_freq = torch.fft.rfft2(target, dim=(-2, -1), norm=\"ortho\")\n",
    "        \n",
    "        # Compute L2 distance in frequency domain\n",
    "        # Consider both magnitude and phase information\n",
    "        loss_real = F.mse_loss(pred_freq.real, target_freq.real, reduction='mean')\n",
    "        loss_imag = F.mse_loss(pred_freq.imag, target_freq.imag, reduction='mean')\n",
    "        \n",
    "        return loss_real + loss_imag\n",
    "\n",
    "class SpectralDualLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, spatial_weight: float = 1.0, freq_weight: float = 0.1,\n",
    "                 use_dice: bool = True, use_focal: bool = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.spatial_weight = spatial_weight\n",
    "        self.freq_weight = freq_weight\n",
    "        self.use_dice = use_dice\n",
    "        self.use_focal = use_focal\n",
    "        \n",
    "        # Spatial losses\n",
    "        if use_dice:\n",
    "            self.dice_loss = DiceLoss(smooth=1e-5)\n",
    "        \n",
    "        if use_focal:\n",
    "            self.focal_loss = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "        else:\n",
    "            self.ce_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Frequency loss\n",
    "        self.freq_loss = FrequencyLoss(weight=freq_weight)\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor,\n",
    "                return_components: bool = False) -> torch.Tensor:\n",
    "        \n",
    "        # Ensure target is on same device as pred\n",
    "        target = target.to(pred.device)\n",
    "        \n",
    "        # Spatial losses\n",
    "        spatial_loss = 0.0\n",
    "        losses_dict = {}\n",
    "        \n",
    "        if self.use_dice:\n",
    "            dice = self.dice_loss(pred, target)\n",
    "            spatial_loss = spatial_loss + dice\n",
    "            losses_dict['dice'] = dice.item()\n",
    "        \n",
    "        if self.use_focal:\n",
    "            focal = self.focal_loss(pred, target)\n",
    "            spatial_loss = spatial_loss + focal\n",
    "            losses_dict['focal'] = focal.item()\n",
    "        else:\n",
    "            ce = self.ce_loss(pred, target)\n",
    "            spatial_loss = spatial_loss + ce\n",
    "            losses_dict['ce'] = ce.item()\n",
    "        \n",
    "        # Frequency loss\n",
    "        # For frequency loss, we need to extract the predicted class (argmax) and compare\n",
    "        pred_probs = torch.softmax(pred, dim=1)\n",
    "        pred_class = torch.argmax(pred_probs, dim=1)  # (B, H, W)\n",
    "        \n",
    "        freq = self.freq_loss(pred_class.float(), target.float())\n",
    "        losses_dict['freq'] = freq.item()\n",
    "        \n",
    "        # Weighted combination\n",
    "        total_loss = (self.spatial_weight * spatial_loss + \n",
    "                     self.freq_weight * freq)\n",
    "        losses_dict['total'] = total_loss.item()\n",
    "        \n",
    "        if return_components:\n",
    "            return total_loss, losses_dict\n",
    "        else:\n",
    "            return total_loss\n",
    "\n",
    "class BoundaryAwareLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, kernel_size: int = 3, weight: float = 1.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.weight = weight\n",
    "    \n",
    "    def _compute_boundaries(self, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Convert to float\n",
    "        mask = mask.float().unsqueeze(1)  # (B, 1, H, W)\n",
    "        \n",
    "        # Compute gradients using Sobel-like operation\n",
    "        kernel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],\n",
    "                                dtype=mask.dtype, device=mask.device)\n",
    "        kernel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]],\n",
    "                                dtype=mask.dtype, device=mask.device)\n",
    "        \n",
    "        kernel_x = kernel_x.view(1, 1, 3, 3)\n",
    "        kernel_y = kernel_y.view(1, 1, 3, 3)\n",
    "        \n",
    "        grad_x = F.conv2d(mask, kernel_x, padding=1)\n",
    "        grad_y = F.conv2d(mask, kernel_y, padding=1)\n",
    "        \n",
    "        # Compute magnitude of gradient\n",
    "        grad_magnitude = torch.sqrt(grad_x ** 2 + grad_y ** 2 + 1e-8)\n",
    "        \n",
    "        # Threshold to get boundary pixels\n",
    "        boundary = (grad_magnitude > 0).float().squeeze(1)\n",
    "        \n",
    "        return boundary\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Get predicted class\n",
    "        pred_probs = torch.softmax(pred, dim=1)\n",
    "        pred_class = torch.argmax(pred_probs, dim=1)  # (B, H, W)\n",
    "        \n",
    "        # Compute boundary maps\n",
    "        pred_boundary = self._compute_boundaries(pred_class)\n",
    "        target_boundary = self._compute_boundaries(target)\n",
    "        \n",
    "        # Compute cross-entropy loss weighted by boundary\n",
    "        ce_loss = F.cross_entropy(pred, target.long(), reduction='none')\n",
    "        \n",
    "        # Apply boundary weight (higher loss for boundary pixels)\n",
    "        boundary_weight = (pred_boundary + target_boundary).clamp(0, 1)\n",
    "        boundary_weight = 1.0 + boundary_weight  # Weight between 1 and 2\n",
    "        \n",
    "        weighted_loss = ce_loss * boundary_weight\n",
    "        \n",
    "        return weighted_loss.mean()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test losses\n",
    "    batch_size, num_classes, height, width = 2, 3, 64, 64\n",
    "    \n",
    "    # Create dummy predictions and targets\n",
    "    pred = torch.randn(batch_size, num_classes, height, width)\n",
    "    target = torch.randint(0, num_classes, (batch_size, height, width))\n",
    "    \n",
    "    # Test SpectralDualLoss\n",
    "    loss_fn = SpectralDualLoss(spatial_weight=1.0, freq_weight=0.1)\n",
    "    loss, components = loss_fn(pred, target, return_components=True)\n",
    "    \n",
    "    print(f\"Total Loss: {loss.item():.4f}\")\n",
    "    for name, value in components.items():\n",
    "        print(f\"  {name}: {value:.4f}\")\n",
    "    \n",
    "    # Test BoundaryAwareLoss\n",
    "    boundary_loss_fn = BoundaryAwareLoss()\n",
    "    boundary_loss = boundary_loss_fn(pred, target)\n",
    "    print(f\"\\nBoundary Loss: {boundary_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a78b3b",
   "metadata": {},
   "source": [
    "### 2.8 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad17bdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from monai.metrics import compute_hausdorff_distance\n",
    "\n",
    "def count_parameters(model):\n",
    "    \n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class SegmentationMetrics:\n",
    "    def __init__(self, num_classes, device):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.batches = 0\n",
    "        self.total_correct_pixels = 0\n",
    "        self.total_pixels = 0\n",
    "        \n",
    "        # Aggregated stats for Precision/Recall/F1 (Global)\n",
    "        self.tp = torch.zeros(self.num_classes, device=self.device)\n",
    "        self.fp = torch.zeros(self.num_classes, device=self.device)\n",
    "        self.fn = torch.zeros(self.num_classes, device=self.device)\n",
    "        \n",
    "        # Accumulators for averaging Batch-wise metrics\n",
    "        self.dice_sum = torch.zeros(self.num_classes, device=self.device)\n",
    "        self.iou_sum = torch.zeros(self.num_classes, device=self.device)\n",
    "        self.hd95_sum = torch.zeros(self.num_classes, device=self.device)\n",
    "        \n",
    "        # Track valid batches for HD95 (it can be NaN if class is missing)\n",
    "        self.hd95_counts = torch.zeros(self.num_classes, device=self.device)\n",
    "\n",
    "    def update(self, preds, targets):\n",
    "        \n",
    "        self.batches += 1\n",
    "        \n",
    "        # Accuracy\n",
    "        self.total_correct_pixels += (preds == targets).sum().item()\n",
    "        self.total_pixels += targets.numel()\n",
    "        \n",
    "        # Create one-hot for HD95 and Dice\n",
    "        # preds_oh: (B, C, H, W)\n",
    "        preds_oh = F.one_hot(preds, num_classes=self.num_classes).permute(0, 3, 1, 2).float()\n",
    "        targets_oh = F.one_hot(targets, num_classes=self.num_classes).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        # Helper for Dice/IoU/TP/FP/FN\n",
    "        for c in range(self.num_classes):\n",
    "            p_flat = preds_oh[:, c].reshape(-1)\n",
    "            t_flat = targets_oh[:, c].reshape(-1)\n",
    "            \n",
    "            intersection = (p_flat * t_flat).sum()\n",
    "            union = p_flat.sum() + t_flat.sum()\n",
    "            \n",
    "            # Global TP/FP/FN accumulation\n",
    "            self.tp[c] += intersection\n",
    "            self.fp[c] += (p_flat.sum() - intersection)\n",
    "            self.fn[c] += (t_flat.sum() - intersection)\n",
    "            \n",
    "            # Batch-wise Dice/IoU accumulation\n",
    "            dice = (2. * intersection + 1e-6) / (union + 1e-6)\n",
    "            iou = (intersection + 1e-6) / (union - intersection + 1e-6)\n",
    "            \n",
    "            self.dice_sum[c] += dice\n",
    "            self.iou_sum[c] += iou\n",
    "            \n",
    "        # HD95 Compliance (MONAI)\n",
    "        # compute_hausdorff_distance expects (B, C, spatial...)\n",
    "        # include_background=True usually, but we iterate.\n",
    "        # We can compute all classes at once.\n",
    "        try:\n",
    "            import warnings\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                \n",
    "                # percentile=95\n",
    "                hd95_batch = compute_hausdorff_distance(\n",
    "                    y_pred=preds_oh, \n",
    "                    y=targets_oh, \n",
    "                    include_background=True, \n",
    "                    percentile=95.0,\n",
    "                    spacing=None  # Pixel space\n",
    "                )\n",
    "            # hd95_batch is (B, C)\n",
    "            \n",
    "            for c in range(self.num_classes):\n",
    "                # Filter NaNs/Infs (happens if class missing in both pred and target, or just one)\n",
    "                # MONAI returns NaN if one is empty. We mostly care if target exists.\n",
    "                # Common practice: if target is empty, skip. If target exists but pred empty, HD is high (inf).\n",
    "                # MONAI behavior: Nan if both empty. Inf if one empty.\n",
    "                \n",
    "                valid_vals = hd95_batch[:, c]\n",
    "                valid_mask = ~torch.isnan(valid_vals) & ~torch.isinf(valid_vals)\n",
    "                \n",
    "                if valid_mask.any():\n",
    "                    self.hd95_sum[c] += valid_vals[valid_mask].sum()\n",
    "                    self.hd95_counts[c] += valid_mask.sum()\n",
    "                    \n",
    "        except Exception as e:\n",
    "            # Fallback or strict error? \n",
    "            # Often happens if shapes are weird or empty batch.\n",
    "            pass\n",
    "\n",
    "    def compute(self):\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # Global Accuracy\n",
    "        metrics['accuracy'] = self.total_correct_pixels / max(self.total_pixels, 1)\n",
    "        \n",
    "        # Per-class metrics\n",
    "        dice_scores = []\n",
    "        iou_scores = []\n",
    "        hd95_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        for c in range(self.num_classes):\n",
    "            # Batch-averaged Dice/IoU\n",
    "            dice_scores.append((self.dice_sum[c] / max(self.batches, 1)).item())\n",
    "            iou_scores.append((self.iou_sum[c] / max(self.batches, 1)).item())\n",
    "            \n",
    "            # Batch-averaged HD95\n",
    "            if self.hd95_counts[c] > 0:\n",
    "                hd95_scores.append((self.hd95_sum[c] / self.hd95_counts[c]).item())\n",
    "            else:\n",
    "                hd95_scores.append(float('nan')) # Or 0.0 or inf\n",
    "            \n",
    "            # Global-based Precision/Recall/F1\n",
    "            p = (self.tp[c] / (self.tp[c] + self.fp[c] + 1e-6)).item()\n",
    "            r = (self.tp[c] / (self.tp[c] + self.fn[c] + 1e-6)).item()\n",
    "            f1 = 2 * p * r / (p + r + 1e-6) if (p + r) > 0 else 0.0\n",
    "            \n",
    "            precision_scores.append(p)\n",
    "            recall_scores.append(r)\n",
    "            f1_scores.append(f1)\n",
    "            \n",
    "        metrics['dice_scores'] = dice_scores\n",
    "        metrics['iou'] = iou_scores\n",
    "        metrics['hd95'] = hd95_scores\n",
    "        metrics['precision'] = precision_scores\n",
    "        metrics['recall'] = recall_scores\n",
    "        metrics['f1_score'] = f1_scores\n",
    "        \n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af3e4086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All modules loaded!\n"
     ]
    }
   ],
   "source": [
    "print('âœ… All modules loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0373d79",
   "metadata": {},
   "source": [
    "---\n",
    "## 3ï¸âƒ£ Dataset\n",
    "\n",
    "Download and preprocess ACDC cardiac MRI dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74be26ee",
   "metadata": {},
   "source": [
    "### 3.1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f804fc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Dataset: ACDC\n",
      "   Classes: ['Background', 'RV', 'Myocardium', 'LV']\n"
     ]
    }
   ],
   "source": [
    "DATASET = 'ACDC'\n",
    "DRIVE_FOLDER_ID = '1EelzBVjIoDQ4uzt0_2JzmF_PuUHsD93e'\n",
    "RAW_DATA_DIR = f'./data/{DATASET}'\n",
    "PREPROCESSED_DIR = f'./preprocessed_data/{DATASET}'\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 4\n",
    "CLASS_NAMES = ['Background', 'RV', 'Myocardium', 'LV']\n",
    "\n",
    "os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
    "print(f\"ðŸ“Š Dataset: {DATASET}\")\n",
    "print(f\"   Classes: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9206647",
   "metadata": {},
   "source": [
    "### 3.2 Download from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0565e031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/samdazel/automated-cardiac-diagnosis-challenge-miccai17?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.11G/2.11G [01:39<00:00, 22.7MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source import complete.\n",
      "Data path: /root/.cache/kagglehub/datasets/samdazel/automated-cardiac-diagnosis-challenge-miccai17/versions/2\n",
      "  database\n",
      "RAW_DATA_DIR: /root/.cache/kagglehub/datasets/samdazel/automated-cardiac-diagnosis-challenge-miccai17/versions/2/database\n",
      "  training\n",
      "  testing\n",
      "  MANDATORY_CITATION.md\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "acdc_path = kagglehub.dataset_download('samdazel/automated-cardiac-diagnosis-challenge-miccai17')\n",
    "\n",
    "print('Data source import complete.')\n",
    "print(f'Data path: {acdc_path}')\n",
    "# Xem cáº¥u trÃºc thÆ° má»¥c\n",
    "import os\n",
    "for item in os.listdir(acdc_path):\n",
    "    print(f'  {item}')\n",
    "\n",
    "\n",
    "RAW_DATA_DIR = os.path.join(acdc_path, 'database')\n",
    "print(f'RAW_DATA_DIR: {RAW_DATA_DIR}')\n",
    "# Xem cáº¥u trÃºc\n",
    "for item in os.listdir(RAW_DATA_DIR):\n",
    "    print(f'  {item}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa89e59",
   "metadata": {},
   "source": [
    "### 3.3 Preprocess (NIfTI â†’ NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8dd8c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Input: /root/.cache/kagglehub/datasets/samdazel/automated-cardiac-diagnosis-challenge-miccai17/versions/2/database/training\n",
      "ðŸ“‚ Output: ./preprocessed_data/ACDC\n",
      "Found 100 patients. Outputting 2D slices to ./preprocessed_data/ACDC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ACDC: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:16<00:00,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed! Saved 1902 slices total.\n",
      "Images: ./preprocessed_data/ACDC/images\n",
      "Masks:  ./preprocessed_data/ACDC/masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import configparser\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Add project root to path\n",
    "\n",
    "def normalize_intensity(image):\n",
    "    \n",
    "    # Clip outliers\n",
    "    p05 = np.percentile(image, 0.5)\n",
    "    p995 = np.percentile(image, 99.5)\n",
    "    image = np.clip(image, p05, p995)\n",
    "    \n",
    "    # Z-score\n",
    "    mean = np.mean(image)\n",
    "    std = np.std(image)\n",
    "    if std > 0:\n",
    "        return (image - mean) / std\n",
    "    return image\n",
    "\n",
    "def preprocess_single_patient_acdc(patient_path, output_dir, target_size=(224, 224)):\n",
    "    \n",
    "    patient_folder = os.path.basename(patient_path)\n",
    "    info_cfg_path = os.path.join(patient_path, 'Info.cfg')\n",
    "    \n",
    "    # Táº¡o folder output cho slice\n",
    "    img_save_dir = os.path.join(output_dir, 'images')\n",
    "    mask_save_dir = os.path.join(output_dir, 'masks')\n",
    "    os.makedirs(img_save_dir, exist_ok=True)\n",
    "    os.makedirs(mask_save_dir, exist_ok=True)\n",
    "    \n",
    "    # Äá»c config Ä‘á»ƒ biáº¿t frame nÃ o lÃ  ED, frame nÃ o lÃ  ES\n",
    "    if not os.path.exists(info_cfg_path):\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        parser = configparser.ConfigParser()\n",
    "        with open(info_cfg_path, 'r') as f:\n",
    "            config_string = '[DEFAULT]\\n' + f.read()\n",
    "        parser.read_string(config_string)\n",
    "        ed_frame = int(parser['DEFAULT']['ED'])\n",
    "        es_frame = int(parser['DEFAULT']['ES'])\n",
    "    except Exception as e:\n",
    "        print(f\"  Error reading Info.cfg for {patient_folder}: {e}\")\n",
    "        return 0\n",
    "    \n",
    "    slices_saved = 0\n",
    "    \n",
    "    for frame_num, frame_name in [(ed_frame, 'ED'), (es_frame, 'ES')]:\n",
    "        img_filename = f'{patient_folder}_frame{frame_num:02d}.nii.gz'\n",
    "        mask_filename = f'{patient_folder}_frame{frame_num:02d}_gt.nii.gz'\n",
    "        \n",
    "        # TÃ¬m file (support cáº£ .nii vÃ  .nii.gz)\n",
    "        img_path = None\n",
    "        mask_path = None\n",
    "        for suffix in ['.gz', '']:\n",
    "            test_img = os.path.join(patient_path, img_filename.replace('.gz', '') if suffix == '' else img_filename)\n",
    "            test_mask = os.path.join(patient_path, mask_filename.replace('.gz', '') if suffix == '' else mask_filename)\n",
    "            if os.path.exists(test_img):\n",
    "                img_path = test_img\n",
    "                mask_path = test_mask\n",
    "                break\n",
    "        \n",
    "        if img_path is None or not os.path.exists(img_path):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Load NIfTI\n",
    "            img_nii = nib.load(img_path)\n",
    "            img_data = img_nii.get_fdata() # (H, W, D)\n",
    "            \n",
    "            mask_data = None\n",
    "            if os.path.exists(mask_path):\n",
    "                mask_data = nib.load(mask_path).get_fdata()\n",
    "            else:\n",
    "                continue # Bá» qua náº¿u khÃ´ng cÃ³ mask\n",
    "            \n",
    "            # 1. Normalize Intensity TRÆ¯á»šC khi resize (tÃ­nh trÃªn toÃ n volume 3D)\n",
    "            img_data = normalize_intensity(img_data)\n",
    "            \n",
    "            num_slices = img_data.shape[2]\n",
    "            \n",
    "            # 2. Xá»­ lÃ½ tá»«ng slice vÃ  lÆ°u ngay láº­p tá»©c\n",
    "            for i in range(num_slices):\n",
    "                slice_img = img_data[:, :, i]\n",
    "                slice_mask = mask_data[:, :, i]\n",
    "                \n",
    "                # Bá» qua slice Ä‘en thui (khÃ´ng cÃ³ thÃ´ng tin) Ä‘á»ƒ trÃ¡nh nhiá»…u training\n",
    "                if np.sum(slice_img) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Resize (LÆ°u Ã½: resize cá»§a skimage range input=output, Ä‘Ã£ normalize thÃ¬ váº«n giá»¯ range)\n",
    "                slice_img_resized = resize(\n",
    "                    slice_img, target_size, order=1, preserve_range=True, anti_aliasing=True, mode='reflect'\n",
    "                ).astype(np.float32)\n",
    "                \n",
    "                slice_mask_resized = resize(\n",
    "                    slice_mask, target_size, order=0, preserve_range=True, anti_aliasing=False, mode='reflect'\n",
    "                ).astype(np.uint8) # Mask pháº£i lÃ  int\n",
    "                \n",
    "                # Táº¡o tÃªn file: patient001_ED_slice005.npy\n",
    "                file_id = f\"{patient_folder}_{frame_name}_slice{i:03d}\"\n",
    "                \n",
    "                np.save(os.path.join(img_save_dir, f\"{file_id}.npy\"), slice_img_resized)\n",
    "                np.save(os.path.join(mask_save_dir, f\"{file_id}.npy\"), slice_mask_resized)\n",
    "                \n",
    "                slices_saved += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {patient_folder} frame {frame_num}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    return slices_saved\n",
    "\n",
    "def preprocess_acdc_dataset(input_dir, output_dir, target_size=(224, 224)):\n",
    "    \n",
    "    # Láº¥y danh sÃ¡ch patient\n",
    "    patient_folders = sorted([\n",
    "        os.path.join(input_dir, d) \n",
    "        for d in os.listdir(input_dir) \n",
    "        if os.path.isdir(os.path.join(input_dir, d)) and d.startswith('patient')\n",
    "    ])\n",
    "    \n",
    "    print(f\"Found {len(patient_folders)} patients. Outputting 2D slices to {output_dir}\")\n",
    "    \n",
    "    total_slices = 0\n",
    "    \n",
    "    for patient_path in tqdm(patient_folders, desc=\"Processing ACDC\"):\n",
    "        slices = preprocess_single_patient_acdc(patient_path, output_dir, target_size)\n",
    "        total_slices += slices\n",
    "        \n",
    "    print(f\"\\nCompleted! Saved {total_slices} slices total.\")\n",
    "    print(f\"Images: {os.path.join(output_dir, 'images')}\")\n",
    "    print(f\"Masks:  {os.path.join(output_dir, 'masks')}\")\n",
    "\n",
    "\n",
    "training_dir = os.path.join(acdc_path, 'database', 'training')\n",
    "output_dir = './preprocessed_data/ACDC'\n",
    "print(f\"ðŸ“‚ Input: {training_dir}\")\n",
    "print(f\"ðŸ“‚ Output: {output_dir}\")\n",
    "preprocess_acdc_dataset(training_dir, output_dir, target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7e6cd",
   "metadata": {},
   "source": [
    "### 3.4 Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e8876d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TRAIN: 1521 2D slices\n",
      "   VAL: 381 2D slices\n",
      "âœ… Memmap DataLoader ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "\n",
    "class ACDCDataset2D(Dataset):\n",
    "    \"\"\"Fast memmap-based dataset for preprocessed 2D slices.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, split='train'):\n",
    "        img_dir = os.path.join(data_dir, 'images')\n",
    "        all_files = sorted(glob.glob(os.path.join(img_dir, '*.npy')))\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        indices = np.random.permutation(len(all_files))\n",
    "        split_idx = int(0.8 * len(all_files))\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.files = [all_files[i] for i in indices[:split_idx]]\n",
    "        else:\n",
    "            self.files = [all_files[i] for i in indices[split_idx:]]\n",
    "        \n",
    "        self.mask_dir = os.path.join(data_dir, 'masks')\n",
    "        print(f\"   {split.upper()}: {len(self.files)} 2D slices\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        mask_path = os.path.join(self.mask_dir, os.path.basename(img_path))\n",
    "        \n",
    "        # memmap for fast disk access\n",
    "        img = np.load(img_path, mmap_mode='r').copy()\n",
    "        seg = np.load(mask_path, mmap_mode='r').copy()\n",
    "        \n",
    "        # Already 2D (H, W) -> add channel dim -> (1, H, W)\n",
    "        return torch.from_numpy(img).unsqueeze(0).float(), torch.from_numpy(seg).long()\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "train_ds = ACDCDataset2D(PREPROCESSED_DIR, 'train')\n",
    "val_ds = ACDCDataset2D(PREPROCESSED_DIR, 'val')\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "print(f\"âœ… Memmap DataLoader ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d238cbc",
   "metadata": {},
   "source": [
    "---\n",
    "## 4ï¸âƒ£ Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9b806",
   "metadata": {},
   "source": [
    "### 4.1 Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53d1a255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… EGM-Net: 9,133,514 parameters\n"
     ]
    }
   ],
   "source": [
    "model = EGMNet(in_channels=1, num_classes=NUM_CLASSES, img_size=IMG_SIZE).to(device)\n",
    "print(f\"âœ… EGM-Net: {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b160f",
   "metadata": {},
   "source": [
    "### 4.2 Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d48bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SpectralDualLoss(spatial_weight=1.0, freq_weight=0.1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a9d3f",
   "metadata": {},
   "source": [
    "### 4.3 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45942c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-535513800.py:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 9,133,514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/381 [00:00<?, ?it/s]/tmp/ipython-input-535513800.py:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [05:58<00:00,  1.06it/s]\n",
      "/tmp/ipython-input-535513800.py:51: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.6124, Dice=0.5176\n",
      "   âœ… Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [05:55<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=0.4977, Dice=0.6052\n",
      "   âœ… Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [05:58<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss=0.4342, Dice=0.6449\n",
      "   âœ… Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [05:56<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss=0.3814, Dice=0.6960\n",
      "   âœ… Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [05:54<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss=0.3510, Dice=0.6771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [05:53<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss=0.3233, Dice=0.7195\n",
      "   âœ… Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [05:53<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss=0.3036, Dice=0.7400\n",
      "   âœ… Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [05:55<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss=0.2881, Dice=0.7424\n",
      "   âœ… Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [05:54<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss=0.2767, Dice=0.7636\n",
      "   âœ… Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [05:52<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss=0.2654, Dice=0.7768\n",
      "   âœ… Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [05:51<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Loss=0.2558, Dice=0.7697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [05:53<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Loss=0.2499, Dice=0.7764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [05:52<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Loss=0.2401, Dice=0.7813\n",
      "   âœ… Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [05:53<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Loss=0.2369, Dice=0.7853\n",
      "   âœ… Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 276/381 [04:16<01:41,  1.04it/s]"
     ]
    }
   ],
   "source": [
    "# === TRAINING ===\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "EARLY_STOP_PATIENCE = 20\n",
    "NUM_CLASSES = 4\n",
    "CLASS_NAMES = {0: 'BG', 1: 'RV', 2: 'MYO', 3: 'LV'}\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "model = EGMNet(in_channels=1, num_classes=NUM_CLASSES, img_size=IMG_SIZE).to(device)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "criterion = SpectralDualLoss(spatial_weight=1.0, freq_weight=0.1).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10)\n",
    "scaler = GradScaler()\n",
    "\n",
    "best_dice = 0.0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "            loss = criterion(pred, masks)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model.eval()\n",
    "    val_dice_sum = 0\n",
    "    val_count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "            pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "            pred_cls = pred.argmax(dim=1)\n",
    "            for c in range(1, NUM_CLASSES):\n",
    "                inter = ((pred_cls == c) & (masks == c)).sum()\n",
    "                union = (pred_cls == c).sum() + (masks == c).sum()\n",
    "                val_dice_sum += (2 * inter / (union + 1e-5)).item()\n",
    "            val_count += 1\n",
    "    \n",
    "    avg_dice = val_dice_sum / (val_count * (NUM_CLASSES - 1))\n",
    "    scheduler.step(avg_dice)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss={train_loss/len(train_loader):.4f}, Dice={avg_dice:.4f}\")\n",
    "    \n",
    "    if avg_dice > best_dice:\n",
    "        best_dice = avg_dice\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"   âœ… Best model saved!\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "    \n",
    "    if epochs_no_improve >= EARLY_STOP_PATIENCE:\n",
    "        print(f\"Early stopping!\")\n",
    "        break\n",
    "\n",
    "print(f\"Training complete! Best Dice: {best_dice:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EVALUATE FUNCTION ===\n",
    "def evaluate_metrics(model, dataloader, device, num_classes):\n",
    "    \"\"\"Evaluate model on a dataloader and return metrics.\"\"\"\n",
    "    model.eval()\n",
    "    metrics_tracker = SegmentationMetrics(num_classes=num_classes, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "            pred_cls = pred.argmax(dim=1)\n",
    "            metrics_tracker.update(pred_cls, masks)\n",
    "    \n",
    "    return metrics_tracker.compute()\n",
    "\n",
    "# === FINAL TEST EVALUATION ===\n",
    "def final_evaluation(model, test_loader, device, num_classes, class_names):\n",
    "    \"\"\"Run final evaluation on test set.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“Š FINAL TEST EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load best model\n",
    "    checkpoint = torch.load('best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "    \n",
    "    metrics = evaluate_metrics(model, test_loader, device, num_classes)\n",
    "    \n",
    "    print(\"\\n--- Per-Class Results ---\")\n",
    "    for c in range(num_classes):\n",
    "        print(f\"   {class_names[c]:<5}: Dice={metrics['dice_scores'][c]:.4f}, \"\n",
    "              f\"IoU={metrics['iou'][c]:.4f}, HD95={metrics['hd95'][c]:.2f}\")\n",
    "    \n",
    "    avg_dice = np.mean(metrics['dice_scores'][1:])\n",
    "    avg_iou = np.mean(metrics['iou'][1:])\n",
    "    avg_hd95 = np.nanmean(metrics['hd95'][1:])\n",
    "    \n",
    "    print(\"\\n--- Summary (Foreground Only) ---\")\n",
    "    print(f\"   Avg Dice: {avg_dice:.4f}\")\n",
    "    print(f\"   Avg IoU:  {avg_iou:.4f}\")\n",
    "    print(f\"   Avg HD95: {avg_hd95:.2f}\")\n",
    "    print(f\"   Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "final_metrics = final_evaluation(model, val_loader, device, NUM_CLASSES, CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4614a502",
   "metadata": {},
   "source": [
    "---\n",
    "## 5ï¸âƒ£ Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b624317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "images, masks = next(iter(val_loader))\n",
    "images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "    pred_cls = pred.argmax(dim=1)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "for i in range(4):\n",
    "    axes[0, i].imshow(images[i, 0].cpu(), cmap='gray')\n",
    "    axes[0, i].set_title('Input')\n",
    "    axes[1, i].imshow(masks[i].cpu(), cmap='jet', vmin=0, vmax=NUM_CLASSES-1)\n",
    "    axes[1, i].set_title('Ground Truth')\n",
    "    axes[2, i].imshow(pred_cls[i].cpu(), cmap='jet', vmin=0, vmax=NUM_CLASSES-1)\n",
    "    axes[2, i].set_title('Prediction')\n",
    "for ax in axes.flatten():\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
